LangChain使构建将外部数据源和计算与LLM连接的应用程序成为可能。在这个快速入门中，我们将介绍几种不同的方法。我们将从一个简单的LLM链开始，它仅依赖于提示模板中的信息来响应。接下来，我们将构建一个检索链，它从单独的数据库中获取数据，并将数据传递到提示模板中。然后，我们将添加聊天历史记录，以创建一个对话检索链。这使您能够以聊天方式与此LLM进行交互，因此它记得以前的问题。最后，我们将构建一个代理——它使用LLM来确定是否需要获取数据以回答问题。我们将在高层次上涵盖所有这些，但所有这些都有大量细节！我们将链接到相关文档。

## 1、LLM链
我们将展示如何使用通过API可用的模型，如OpenAI。

### （1）安装LangChain x OpenAI集成包

首先，我们需要安装LangChain x OpenAI集成包，它允许我们方便地使用OpenAI提供的模型。

```bash
pip install langchain-openai
```

这一步骤通过Python包管理器pip安装必需的软件包，让我们的Python环境具备访问OpenAI API的能力。

### （2）设置API密钥

要使用OpenAI的API，您需要一个API密钥。此密钥可通过创建OpenAI平台账户后从[此处](https://platform.openai.com/account/api-keys)获取。获取API密钥后，我们推荐将其设置为环境变量，以增加使用的安全性和便利性。

```python
export OPENAI_API_KEY="您的API密钥"
```

此命令将API密钥保存为环境变量，这样在使用API时就不需要在代码中直接暴露密钥。

### （3）初始化模型

有两种方法初始化LangChain x OpenAI集成的模型，取决于您是否设置了环境变量。

#### 1）使用环境变量

如果您已经设置了环境变量，可以直接初始化模型，无需在代码中指定API密钥。

```python
from langchain_openai import ChatOpenAI
llm = ChatOpenAI()
```

#### 2）直接传递API密钥

如果您选择不使用环境变量，也可以在初始化模型时直接传递API密钥。

```python
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(openai_api_key="您的API密钥")
```

#### 3）配置国内跳转address

```python
import os

os.environ["OPENAI_API_BASE"] = 'https://hk.xty.app/v1'
os.environ["OPENAI_API_KEY"] = 'sk-***'
```

### （4）使用模型提问

安装并初始化模型后，我们可以尝试提出问题。这里，我们将询问一个可能不在训练数据中的问题，以查看模型如何响应。

```python
llm.invoke("LangSmith能如何帮助测试？")
```

### （5）使用提示模板

为了改善模型的响应，我们可以使用提示模板。提示模板帮助我们将用户输入转换为模型能更好理解的形式。

```python
from langchain_core.prompts import ChatPromptTemplate
prompt = ChatPromptTemplate.from_messages([
    ("system", "你是一名世界级的技术文档编写者。"),
    ("user", "{input}"),
])
```

此代码创建了一个提示模板，指导模型将其自身视为一名世界级的技术文档编写者，从而提高了回答问题的相关性和质量。

### （6）创建并使用LLM链

接下来，我们将提示模板和模型结合起来，形成一个简单的LLM链。这使我们能够以更结构化的方式使用模型和模板。

```python
chain = prompt | llm 
```

然后我们可以再次提出之前的问题。它仍然不会知道答案，但它应该以更适当的技术作家的语气回应！

```python
chain.invoke({"input": "LangSmith能如何帮助测试？"})
```

### （7）添加输出解析器

为了让输出更加易于理解和使用，我们添加一个将聊天消息转换为字符串的输出解析器。

```python
from langchain_core.output_parsers import StrOutputParser
output_parser = StrOutputParser()
```

将输出解析器添加到LLM链中：

```python
chain = prompt | llm | output_parser
```

现在，当我们再次提出问题时，得到的答案将直接是一个字符串，更加便于阅读和处理。

```python
chain.invoke({"input": "LangSmith能如何帮助测试？"})
```

## 2、检索链
为了正确回答原始问题（"LangSmith能如何帮助测试？"），我们需要为LLM提供额外的上下文。我们可以通过检索来实现这一点。当您有**太多数据**无法直接传递给LLM时，检索非常有用。然后您可以使用检索器仅获取最相关的片段并将它们传递进去。

在此过程中，我们将从_检索器_中查找相关文档，然后将它们传递到提示中。检索器可以由任何东西支持——SQL表、互联网等——但在此实例中，我们将填充一个向量存储并将其用作检索器。

### （1）准备数据

首先，我们需要加载我们想要索引的数据。为了做到这一点，我们将使用WebBaseLoader。这需要安装[BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/)：

```bash
pip install beautifulsoup4
```

之后，我们可以导入并使用WebBaseLoader。

```python
from langchain_community.document_loaders import WebBaseLoader
loader = WebBaseLoader("https://docs.smith.langchain.com/user_guide")
docs = loader.load()
```

### （2）构建索引

有了数据之后，接下来的步骤是将其索引到向量存储中，这需要一个嵌入模型和向量存储库。

#### 1）嵌入模型

首先，确保已安装`langchain_openai`包，并设置了API环境变量。然后，我们使用OpenAI的嵌入模型来处理文档：

```python
from langchain_openai import OpenAIEmbeddings
embeddings = OpenAIEmbeddings()
```

#### 2）索引到向量存储

我们选择使用FAISS作为本地向量存储。首先，需要安装FAISS包（安装命令未显示）。然后，使用嵌入模型将文档内容转换为向量，并将这些向量存储到FAISS中：

```python
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter()
documents = text_splitter.split_documents(docs)
vector = FAISS.from_documents(documents, embeddings)
```

### （3）创建检索链

现在，我们有了索引的数据，接下来是创建一个检索链。这个链负责接收一个问题，从向量存储中检索到相关的文档，并将这些文档连同问题一起传递给LLM以获取答案。

#### 1）设置检索链

我们首先创建一个处理问题和检索到的文档，生成答案的链：

```python
from langchain.chains.combine_documents import create_stuff_documents_chain
prompt = ChatPromptTemplate.from_template("""根据提供的上下文回答以下问题：
<context>{context}</context>问题：{input}""")
document_chain = create_stuff_documents_chain(llm, prompt)
```

如果我们希望，可以通过直接传递特定文档来测试这个链：

```python
from langchain_core.documents import Document

document_chain.invoke({
    "input": "LangSmith能如何帮助测试？",
    "context": [Document(page_content="LangSmith可以帮助您可视化测试结果")]
})
```

但我们的目标是让文档通过我们之前设置的检索器动态获取，这样对于每个问题，都能选择最相关的文档进行回答。

#### 2）运行检索链

通过将检索器与文档处理链结合，我们创建了一个完整的检索链：

```python
from langchain.chains import create_retrieval_chain
retriever = vector.as_retriever()
retrieval_chain = create_retrieval_chain(retriever, document_chain)
```

现在，我们可以调用这个链来获取问题的答案：

```python
response = retrieval_chain.invoke({"input": "LangSmith能如何帮助测试？"})
print(response["answer"])
```

这个答案应该更准确！

## 3、对话检索链

在创建对话检索链的过程中，我们的目标是将单次回答的能力扩展到能够处理连续对话的聊天机器人。为了实现这一点，我们需要对检索链进行一些关键的更新，以确保它能够考虑到对话的历史。

### （1）更新检索以考虑对话历史

在对话型应用中，回答后续问题时不仅要考虑最近的输入，还需要考虑整个对话历史。这需要我们对检索方式进行更新。

#### 1）创建历史感知检索器

首先，我们创建一个新的链，这个链能够接收最近的输入和整个对话历史，并据此生成搜索查询：

```python
from langchain.chains import create_history_aware_retriever
from langchain_core.prompts import MessagesPlaceholder
# 我们需要一个提示模板，用于生成基于对话历史的搜索查询
prompt = ChatPromptTemplate.from_messages([
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{input}"),
    ("user", "根据以上对话，生成一个搜索查询以查找与对话相关的信息"),
])
retriever_chain = create_history_aware_retriever(llm, retriever, prompt)
```

这个链利用了LLM的能力，结合当前的对话历史生成相关的搜索查询。

#### 2）测试功能

我们可以通过一个实例测试这个新的检索链，模拟用户询问后续问题的场景：

```python
from langchain_core.messages import HumanMessage, AIMessage
chat_history = [HumanMessage(content="LangSmith能帮助测试我的LLM应用吗？"), AIMessage(content="可以！")]
retriever_chain.invoke({
    "chat_history": chat_history,
    "input": "告诉我怎么做",
})
```

此时，您应该得到了与LangSmith测试相关的文档信息，说明LLM已经根据对话历史和后续问题生成了新的搜索查询。

### （2）创建考虑检索文档的对话链

有了基于历史的检索器后，我们需要创建一个新的链，使其能够考虑到检索到的文档，并继续进行对话。

```python
prompt = ChatPromptTemplate.from_messages([
    ("system", "根据下方上下文回答用户的问题：\n\n{context}"),
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{input}"),
])
document_chain = create_stuff_documents_chain(llm, prompt)
retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)
```

这个链综合了对话历史和检索到的文档，为用户提供更加准确和连贯的回答。

### （3）从头到尾测试聊天功能

现在，我们可以完整地测试这个功能，模拟一个连续的对话过程：

```python
chat_history = [HumanMessage(content="LangSmith能帮助测试我的LLM应用吗？"), AIMessage(content="可以！")]
retrieval_chain.invoke({
    "chat_history": chat_history,
    "input": "告诉我怎么做",
})
```

通过这个流程，我们可以看到系统提供了一个连贯的答案，展示了我们已经成功地将检索链转变为一个能够处理连续对话的聊天机器人。这不仅使得回答更加贴合上下文，也大大提升了用户体验。

## 4、代理

到目前为止，我们创建了链的示例——其中每个步骤都是提前知道的。我们最后要创建的是一个代理——其中LLM决定采取什么步骤。

**注意：对于此示例，我们将仅展示如何使用OpenAI模型创建代理，因为本地模型还不够可靠。**

构建代理的第一件事是决定它应该可以访问哪些工具。对于此示例，我们将让代理访问两个工具：

1. 我们刚刚创建的检索器。这将让它轻松回答有关LangSmith的问题
2. 一个搜索工具。这将让它轻松回答需要最新信息的问题。

首先，让我们为刚刚创建的检索器设置一个工具：

```python
from langchain.tools.retriever import create_retriever_tool
retriever_tool = create_retriever_tool(
    retriever,
    "langsmith_search",
    "Search for information about LangSmith. For any questions about LangSmith, you must use this tool!",
)
```

我们将使用的搜索工具是[Tavily](https://python.langchain.com/docs/integrations/retrievers/tavily)。这将需要一个API密钥（他们有慷慨的免费层级）。在他们的平台上创建后，您需要将其设置为环境变量：

```python
export TAVILY_API_KEY=...
```

如果您不想设置API密钥，可以跳过创建此工具。

```python
from langchain_community.tools.tavily_search import TavilySearchResults
search = TavilySearchResults()
```

我们现在可以创建我们想要使用的工具列表：

```python
tools = [retriever_tool, search]
```

现在我们有了工具，我们可以创建一个代理来使用它们。

首先安装langchain hub

```bash
pip install langchainhub
```

现在我们可以使用它来获取预定义的提示

```python
from langchain_openai import ChatOpenAI
from langchain import hub
from langchain.agents import create_openai_functions_agent
from langchain.agents import AgentExecutor
# 获取要使用的提示 - 您可以修改这个！
prompt = hub.pull("hwchase17/openai-functions-agent")
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
agent = create_openai_functions_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
```

我们现在可以调用代理并查看它的响应！我们可以询问有关LangSmith的问题：

```python
agent_executor.invoke({"input": "how can langsmith help with testing?"})
```

我们可以询问天气：

```python
agent_executor.invoke({"input": "what is the weather in SF?"})
```

我们可以与它进行对话：

```python
chat_history = [HumanMessage(content="Can LangSmith help test my LLM applications?"), AIMessage(content="Yes!")]
agent_executor.invoke({
    "chat_history": chat_history,
    "input": "Tell me how",
})
```

## 5、使用LangServe提供服务

现在我们已经构建了一个应用程序，我们需要提供它。这就是LangServe的用处。LangServe帮助开发者将LangChain链作为REST API部署。您不需要使用LangServe来使用LangChain，但在此指南中，我们将展示如何使用LangServe部署您的应用程序。

虽然本指南的第一部分旨在在Jupyter笔记本中运行，但我们现在将从中移出。我们将创建一个Python文件，然后从命令行与之交互。

安装：

```bash
pip install "langserve[all]"
```

### 服务器

为了为我们的应用程序创建一个服务器，我们将制作一个`serve.py`文件。这将包含我们为提供应用程序而编写的逻辑。它包括三件事：

1. 我们刚刚构建的链的定义
2. 我们的FastAPI应用程序
3. 一个从`langserve.add_routes`定义的路由，用于提供链

```python
#!/usr/bin/env python
from typing import List

from fastapi import FastAPI
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import WebBaseLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.tools.retriever import create_retriever_tool
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain import hub
from langchain.agents import create_openai_functions_agent
from langchain.agents import AgentExecutor
from langchain.pydantic_v1 import BaseModel, Field
from langchain_core.messages import BaseMessage
from langserve import add_routes

# 1. 加载检索器
loader = WebBaseLoader("https://docs.smith.langchain.com/user_guide")
docs = loader.load()
text_splitter = RecursiveCharacterTextSplitter()
documents = text_splitter.split_documents(docs)
embeddings = OpenAIEmbeddings()
vector = FAISS.from_documents(documents, embeddings)
retriever = vector.as_retriever()

# 2. 创建工具
retriever_tool = create_retriever_tool(
    retriever,
    "langsmith_search",
    "Search for information about LangSmith. For any questions about LangSmith, you must use this tool!",
)
search = TavilySearchResults()
tools = [retriever_tool, search]


# 3. 创建代理
prompt = hub.pull("hwchase17/openai-functions-agent")
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
agent = create_openai_functions_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)


# 4. 应用程序定义
app = FastAPI(
  title="LangChain Server",
  version="1.0",
  description="A simple API server using LangChain's Runnable interfaces",
)

# 5. 添加链路由

# 我们需要添加这些输入/输出模式，因为当前的AgentExecutor缺乏模式。

class Input(BaseModel):
    input: str
    chat_history: List[BaseMessage] = Field(
        ...,
        extra={"widget": {"type": "chat", "input": "location"}},
    )


class Output(BaseModel):
    output: str

add_routes(
    app,
    agent_executor.with_types(input_type=Input, output_type=Output),
    path="/agent",
)

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=8000)
```

就是这样！如果我们执行这个文件： 

```bash
python serve.py
```

我们应该会在localhost:8000看到我们的链被提供服务。 

### Playground

每个LangServe服务都配备了一个简单的内置UI，用于配置和调用应用程序，具有流式输出和对中间步骤的可见性。前往http://localhost:8000/agent/playground/尝试一下！传递与之前相同的问题——"how can langsmith help with testing?"——它应该像之前一样响应。 

### 客户端

现在让我们设置一个客户端，以编程方式与我们的服务进行交互。我们可以使用`langserve.RemoteRunnable`轻松完成这一点。使用这个，我们可以与提供的链进行交互，就像它是在客户端运行一样。

```python
from langserve import RemoteRunnable

remote_chain = RemoteRunnable("http://localhost:8000/agent/")
remote_chain.invoke({
    "input": "how can langsmith help with testing?",
    "chat_history": []  # Providing an empty list as this is the first call
})
```

