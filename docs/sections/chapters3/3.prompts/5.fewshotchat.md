少样本提示模板的目标是基于输入动态选择示例，然后将示例格式化到最终提示中，以便为模型提供参考。

### 1、固定示例
最基本的（也是最常见的）少样本提示技术是使用固定的提示示例。这样可以选择一个链条，评估它，并避免担心生产中额外的移动部分。

模板的基本组件包括：
- `examples`：要在最终提示中包含的字典示例列表。
- `example_prompt`：通过其`format_messages`方法将每个示例转换为1个或多个消息。一个常见的例子是将每个示例转换为一个人类消息和一个AI消息响应，或者一个人类消息后跟一个函数调用消息。

下面是一个简单的演示。首先，导入这个示例所需的模块：

```python
from langchain.prompts import (
    ChatPromptTemplate,
    FewShotChatMessagePromptTemplate,
)
```

然后，定义你想包含的示例。

```python
examples = [
    {"input": "2+2", "output": "4"},
    {"input": "2+3", "output": "5"},
]
```

接下来，将它们组装到少样本提示模板中。

```python
# 这是一个用于格式化每个单独示例的提示模板。
example_prompt = ChatPromptTemplate.from_messages(
    [
        ("human", "{input}"),
        ("ai", "{output}"),
    ]
)
few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_prompt=example_prompt,
    examples=examples,
)
print(few_shot_prompt.format())
```

```
Human: 2+2 AI: 4
Human: 2+3 AI: 5
```

最后，组装你的最终提示，并将其与模型一起使用。

```python
final_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a wondrous wizard of math."),
        few_shot_prompt,
        ("human", "{input}"),
    ]
)
```

```python
from langchain_openai import ChatOpenAI

chat_model = ChatOpenAI(model="gpt-3.5-turbo")

chain = final_prompt | chat_model

print(chain.invoke({"input": "What's the square of a triangle?"}))
```

```text
Human: 2+2
AI: 4
Human: 2+3
AI: 5
content='A square is a type of quadrilateral with four equal sides and four right angles. A triangle is a polygon with three sides. Therefore, a square does not have a square - it has an area.' response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 52, 'total_tokens': 93}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}
```

### 2、动态少样本提示
有时，你可能希望根据输入来决定显示哪些示例。为此，你可以用`example_selector`替换`examples`。其他组件与上面相同！
回顾一下，动态少样本提示模板将如下所示：

- `example_selector`：负责为给定输入选择少样本示例（以及它们返回的顺序）。这些实现了`BaseExampleSelector`接口。一个常见的例子是由向量存储支持的`SemanticSimilarityExampleSelector`。
- `example_prompt`：通过其`format_messages`方法将每个示例转换为1个或多个消息。一个常见的例子是将每个示例转换为一个人类消息和一个AI消息响应，或者一个人类消息后跟一个函数调用消息。

这些可以再次与其他消息和聊天模板组合，以组装你的最终提示。

```python
from langchain.prompts import SemanticSimilarityExampleSelector
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
```

由于我们使用向量存储根据语义相似性选择示例，我们将首先填充存储。

```python
examples = [
    {"input": "2+2", "output": "4"},
    {"input": "2+3", "output": "5"},
    {"input": "2+4", "output": "6"},
    {"input": "What did the cow say to the moon?", "output": "nothing at all"},
    {
        "input": "Write me a poem about the moon",
        "output": "One for the moon, and one for me, who are we to talk about the moon?"
    },
]
to_vectorize = [" ".join(example.values()) for example in examples]
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)
```

#### （1）创建`example_selector`
创建了向量存储后，你可以创建`example_selector`。在这里，我们将指示它只获取前2个示例。

```python
example_selector = SemanticSimilarityExampleSelector(
    vectorstore=vectorstore,
    k=2,
)
# 提示模板将通过传递输入到`select_examples`方法来加载示例
example_selector.select_examples({"input": "horse"})
```

```
[
 {'input': 'What did the cow say to the moon?', 'output': 'nothing at all'},
 {'input': '2+4', 'output': '6'}
]
```

#### （2）创建提示模板
使用上面创建的`example_selector`组装提示模板。

```python
from langchain.prompts import (
    ChatPromptTemplate,
    FewShotChatMessagePromptTemplate,
)
# 定义少样本提示。
few_shot_prompt = FewShotChatMessagePromptTemplate(
    # 输入变量选择传递给`example_selector`的值。
    input_variables=["input"],
    example_selector=example_selector,
    # 定义每个示例如何格式化。
    # 在这种情况下，每个示例将成为2条消息：
    # 1个人类，和1个AI
    example_prompt=ChatPromptTemplate.from_messages(
        [
            ("human", "{input}"),
            ("ai", "{output}"),
        ]
    ),
)
```

下面是如何组装这个模板的示例。

```python
print(few_shot_prompt.format(input="What's 3+3?"))
```

```
Human: 2+3 AI: 5
Human: 2+2 AI: 4
```

组装最终的提示模板：

```python
final_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a wondrous wizard of math."),
        few_shot_prompt,
        ("human", "{input}"),
    ]
)
```

```python
print(few_shot_prompt.format(input="What's 3+3?"))
```

```
Human: 2+3 AI: 5
Human: 2+2 AI: 4
```

#### （3）与LLM一起使用
现在，你可以将你的模型连接到少样本提示。

```python
from langchain_community.chat_models import ChatAnthropic
chain = final_prompt | ChatAnthropic(temperature=0.0)
chain.invoke({"input": "What's 3+3?"})
```

```
AIMessage(content=' 3 + 3 = 6', additional_kwargs={}, example=False)
```