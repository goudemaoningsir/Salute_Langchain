## 1、CSV解析器

当您想要返回一个逗号分隔的项目列表时，可以使用这个输出解析器。

```python
from langchain.output_parsers import CommaSeparatedListOutputParser
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

output_parser = CommaSeparatedListOutputParser()
format_instructions = output_parser.get_format_instructions()

prompt = PromptTemplate(
    template="列出五个{subject}。\n{format_instructions}",
    input_variables=["subject"],
    partial_variables={"format_instructions": format_instructions},
)

model = ChatOpenAI(temperature=0)
chain = prompt | model | output_parser
print(
    chain.invoke({"subject": "ice cream flavors"})
)  # ['Vanilla', 'Chocolate', 'Strawberry', 'Mint Chocolate Chip', 'Cookies and Cream']
for s in chain.stream({"subject": "ice cream flavors"}):
    print(
        s
    )  # ['Vanilla']['Chocolate']['Strawberry']['Mint Chocolate Chip']['Cookies and Cream']
```

## 2、日期时间解析器

这个输出解析器可以用来将LLM输出解析为日期时间格式。

```python
from langchain.output_parsers import DatetimeOutputParser
from langchain.prompts import PromptTemplate
from langchain_openai import OpenAI
output_parser = DatetimeOutputParser()
template = "Answer the users question:{question}{format_instructions}"
prompt = PromptTemplate.from_template(
    template,
    partial_variables={"format_instructions": output_parser.get_format_instructions()},
)
PromptTemplate(input_variables=['question'], partial_variables={'format_instructions': "Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\n\nExamples: 0668-08-09T12:56:32.732651Z, 1213-06-23T21:01:36.868629Z, 0713-07-06T18:19:02.257488Z\n\nReturn ONLY this string, no other words!"}, template='Answer the users question:\n\n{question}\n\n{format_instructions}')
chain = prompt | OpenAI() | output_parser
output = chain.invoke({"question": "when was bitcoin founded?"})
```

## 3、枚举解析器

这个笔记本展示了如何使用枚举输出解析器。

```python
from langchain.output_parsers.enum import EnumOutputParser
from enum import Enum


class Colors(Enum):
    RED = "red"
    GREEN = "green"
    BLUE = "blue"


parser = EnumOutputParser(enum=Colors)


from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

prompt = PromptTemplate.from_template(
    """What color eyes does this person have?\n> Person: {person}\nInstructions: {instructions}""",
).partial(instructions=parser.get_format_instructions())

chain = prompt | ChatOpenAI() | parser

print(chain.invoke({"person": "Frank Sinatra"}))
```

## 4、JSON解析器

这个输出解析器允许用户指定任意的JSON模式，并查询LLM以获取符合该模式的输出。

请记住，大型语言模型是泄露的抽象！您需要使用具有足够容量的LLM来生成格式良好的JSON。在OpenAI系列中，DaVinci可以可靠地做到这一点，但Curie的能力已经大幅下降。

您可以选择使用Pydantic来声明您的数据模型。

```python
from typing import List

from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI

model = ChatOpenAI(temperature=0)


# Define your desired data structure.
class Joke(BaseModel):
    setup: str = Field(description="question to set up a joke")
    punchline: str = Field(description="answer to resolve the joke")


# And a query intented to prompt a language model to populate the data structure.
joke_query = "Tell me a joke."

# Set up a parser + inject instructions into the prompt template.
parser = JsonOutputParser(pydantic_object=Joke)

prompt = PromptTemplate(
    template="Answer the user query.\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)

chain = prompt | model | parser

print(chain.invoke({"query": joke_query}))
```

这个输出解析器支持流式传输。

```python
for s in chain.stream({"query": joke_query}):
    print(s)
```

您也可以在没有Pydantic的情况下使用它。这将提示它返回JSON，但不会提供有关模式应该是什么的详细信息。

```python
joke_query = "Tell me a joke."

parser = JsonOutputParser()

prompt = PromptTemplate(
    template="Answer the user query.\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)

chain = prompt | model | parser

chain.invoke({"query": joke_query})
```

## 5、OpenAI Functions

这些输出解析器使用 OpenAI 函数调用来结构化其输出。这意味着它们只能与支持函数调用的模型一起使用。有几个不同的变体：

- JsonOutputFunctionsParser：将函数调用的参数作为JSON返回
- PydanticOutputFunctionsParser：将函数调用的参数作为Pydantic模型返回
- JsonKeyOutputFunctionsParser：将函数调用中特定键的值作为JSON返回
- PydanticAttrOutputFunctionsParser：将函数调用中特定键的值作为Pydantic模型返回

```python
from langchain_community.utils.openai_functions import (
    convert_pydantic_to_openai_function,
)
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field, validator
from langchain_openai import ChatOpenAI
```

```python
class Joke(BaseModel):
    """Joke to tell user."""

    setup: str = Field(description="question to set up a joke")
    punchline: str = Field(description="answer to resolve the joke")


openai_functions = [convert_pydantic_to_openai_function(Joke)]
model = ChatOpenAI(temperature=0)

prompt = ChatPromptTemplate.from_messages(
    [("system", "You are helpful assistant"), ("user", "{input}")]
)
```

### （1）JsonOutputFunctionsParser

```python
from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser
parser = JsonOutputFunctionsParser()
chain = prompt | model.bind(functions=openai_functions) | parser
print(chain.invoke({"input": "tell me a joke"}))
for s in chain.stream({"input": "tell me a joke"}):
    print(s)
```

### （2）JsonKeyOutputFunctionsParser

这仅仅从返回的响应中提取单个键。当您想要返回一系列事物时，这非常有用。

```python
from typing import List

from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser


class Jokes(BaseModel):
    """Jokes to tell user."""

    joke: List[Joke]
    funniness_level: int


parser = JsonKeyOutputFunctionsParser(key_name="joke")

openai_functions = [convert_pydantic_to_openai_function(Jokes)]
chain = prompt | model.bind(functions=openai_functions) | parser
print(chain.invoke({"input": "tell me two jokes"}))

for s in chain.stream({"input": "tell me two jokes"}):
    print(s)
```

### （3）PydanticOutputFunctionsParser

这在 `JsonOutputFunctionsParser` 的基础上构建，但将结果传递给 Pydantic 模型。如果您选择，这允许进一步验证。

```python
from langchain.output_parsers.openai_functions import PydanticOutputFunctionsParser


class Joke(BaseModel):
    """Joke to tell user."""

    setup: str = Field(description="question to set up a joke")
    punchline: str = Field(description="answer to resolve the joke")

    # You can add custom validation logic easily with Pydantic.
    @validator("setup")
    def question_ends_with_question_mark(cls, field):
        if field[-1] != "?":
            raise ValueError("Badly formed question!")
        return field


parser = PydanticOutputFunctionsParser(pydantic_schema=Joke)
openai_functions = [convert_pydantic_to_openai_function(Joke)]
chain = prompt | model.bind(functions=openai_functions) | parser
print(chain.invoke({"input": "tell me a joke"}))
```

## 6、OpenAI Tools

这些输出解析器从 OpenAI 的函数调用 API 响应中提取工具调用。这意味着它们只能与支持函数调用的模型一起使用，特别是最新的 `tools` 和 `tool_choice` 参数。

有几种不同的输出解析器变体：

- JsonOutputToolsParser：将函数调用的参数作为 JSON 返回
- JsonOutputKeyToolsParser：将函数调用中特定键的值作为 JSON 返回
- PydanticToolsParser：将函数调用的参数作为 Pydantic 模型返回

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field, validator
from langchain_openai import ChatOpenAI


class Joke(BaseModel):
    """要告诉用户的笑话。"""

    setup: str = Field(description="设置笑话的问题")
    punchline: str = Field(description="解决笑话的答案")


model = ChatOpenAI(model="gpt-3.5-turbo", temperature=0).bind_tools([Joke])
print(model.kwargs["tools"])
```

```python
[{'type': 'function', 'function': {'name': 'Joke', 'description': '要告诉用户的笑话。', 'parameters': {'type': 'object', 'properties': {'setup': {'description': '设置笑话的问题', 'type': 'string'}, 'punchline': {'description': '解决笑话的答案', 'type': 'string'}}, 'required': ['setup', 'punchline']}}}]
```

```python
prompt = ChatPromptTemplate.from_messages(
    [("system", "You are helpful assistant"), ("user", "{input}")]
)
```
### （1）JsonOutputToolsParser

```python
from langchain.output_parsers.openai_tools import JsonOutputToolsParser

parser = JsonOutputToolsParser()
chain = prompt | model | parser
print(chain.invoke({"input": "tell me a joke"}))
```

```python
[{'type': 'Joke', 'args': {'setup': "Why couldn't the bicycle stand up by itself?", 'punchline': 'It was two tired!'}}]
```

为了包括工具调用 ID，我们可以指定 `return_id=True`：

```python
parser = JsonOutputToolsParser(return_id=True)
chain = prompt | model | parser
chain.invoke({"input": "告诉我一个笑话"})
```

```python
[{'type': 'Joke', 'args': {'setup': "科学家为什么不相信原子？", 'punchline': '因为它们构成了一切！'}, 'id': 'call_Isuoh0RTeQzzOKGg5QlQ7UqI'}]
```

### （2）JsonOutputKeyToolsParser

这仅仅从返回的响应中提取单个键。当您传递单个工具并只想要它的参数时，这非常有用。

```python
from typing import List

from langchain.output_parsers.openai_tools import JsonOutputKeyToolsParser

parser = JsonOutputKeyToolsParser(key_name="Joke")

chain = prompt | model | parser
print(chain.invoke({"input": "tell me a joke"}))
```

```python
{'setup': "Why don't scientists trust atoms?", 'punchline': 'Because they make up everything!'}
```

某些模型可以在每次调用中返回多个工具调用，因此默认情况下输出是一个列表。如果我们只想返回第一个工具调用，我们可以指定 `first_tool_only=True`

```python
parser = JsonOutputKeyToolsParser(key_name="Joke", first_tool_only=True)
chain = prompt | model | parser
chain.invoke({"input": "告诉我一个笑话"})
```

```python
{'setup': "科学家为什么不相信原子？", 'punchline': '因为它们构成了一切！'}
```

### （3）PydanticToolsParser

这在 `JsonOutputToolsParser` 的基础上构建，但将结果传递给 Pydantic 模型。如果您选择，这允许进一步验证。

```python
from langchain.output_parsers.openai_tools import PydanticToolsParser
class Joke(BaseModel):
    """要告诉用户的笑话。"""
    setup: str = Field(description="设置笑话的问题")
    punchline: str = Field(description="解决笑话的答案")
    # 您可以使用 Pydantic 轻松添加自定义验证逻辑。
    @validator("setup")
    def question_ends_with_question_mark(cls, field):
        if field[-1] != "?":
            raise ValueError("问题格式不正确！")
        return field
parser = PydanticToolsParser(tools=[Joke])

model = ChatOpenAI(model="gpt-3.5-turbo", temperature=0).bind_tools([Joke])
chain = prompt | model | parser
chain.invoke({"input": "告诉我一个笑话"})
```

```python
[Joke(setup='为什么程序员总是在深夜工作？', punchline='因为他们喜欢在黑暗中调试！')]
```

## 7、Output-fixing parser

这个输出解析器包装了另一个输出解析器，在第一个解析器失败的情况下，它会调用另一个LLM来修复任何错误。

但我们可以做的不仅仅是抛出错误。具体来说，我们可以传递格式错误的输出，以及格式化指令，给模型，并要求它修复。

在这个例子中，我们将使用上述的Pydantic输出解析器。如果我们传递一个不符合模式的结果给它，会发生什么呢？

```python
from typing import List

from langchain.output_parsers import PydanticOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI
class Actor(BaseModel):
    name: str = Field(description="name of an actor")
    film_names: List[str] = Field(description="list of names of films they starred in")


actor_query = "Generate the filmography for a random actor."

parser = PydanticOutputParser(pydantic_object=Actor)
misformatted = "{'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}"

parser.parse(misformatted)
```

现在我们可以构建并使用一个`OutputFixingParser`。这个输出解析器接受另一个输出解析器作为参数，但也接受一个LLM，用来尝试纠正任何格式错误。

```python
from langchain.output_parsers import OutputFixingParser

new_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI())

print(new_parser.parse(misformatted))
```

## 8、Pandas DataFrame 解析器

Pandas DataFrame 是 Python 编程语言中的一种流行数据结构，通常用于数据操作和分析。它为处理结构化数据提供了一套全面的 tools，使其成为数据清洗、转换和分析等任务的多功能选择。

这个输出解析器允许用户指定任意的 Pandas DataFrame，并查询 LLM 以获取以格式化字典形式的数据，该字典从相应的 DataFrame 中提取数据。请记住，大型语言模型是泄露的抽象！您必须使用具有足够容量的 LLM 来生成符合定义格式指令的构造良好的查询。

使用 Pandas 的 DataFrame 对象来声明您希望执行查询的 DataFrame。

```python
import pprint
from typing import Any, Dict
import pandas as pd
from langchain.output_parsers import PandasDataFrameOutputParser
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

model = ChatOpenAI(temperature=0)


# 仅出于文档目的。
def format_parser_output(parser_output: Dict[str, Any]) -> None:
    for key in parser_output.keys():
        parser_output[key] = parser_output[key].to_dict()
    return pprint.PrettyPrinter(width=4, compact=True).pprint(parser_output)


# 定义您希望的 Pandas DataFrame。
df = pd.DataFrame(
    {
        "num_legs": [2, 4, 8, 0],
        "num_wings": [2, 0, 0, 0],
        "num_specimen_seen": [10, 2, 1, 8],
    }
)  # 设置解析器 + 将指令注入提示模板。
parser = PandasDataFrameOutputParser(dataframe=df)
# 这里是执行列操作的一个例子。
df_query = "检索 num_wings 列。"
# 设置提示。
prompt = PromptTemplate(
    template="回答用户查询。\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)
chain = prompt | model | parser
parser_output = chain.invoke({"query": df_query})
format_parser_output(parser_output)
```

输出

```python
{'num_wings': {0: 2,
               1: 0,
               2: 0,
               3: 0}}
```

```python
# 这里是执行行操作的一个例子。
df_query = "检索第一行。"
# 设置提示。
prompt = PromptTemplate(
    template="回答用户查询。\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)
chain = prompt | model | parser
parser_output = chain.invoke({"query": df_query})
format_parser_output(parser_output)
```

```python
{'0': {'num_legs': 2,
       'num_specimen_seen': 10,
       'num_wings': 2}}
```

```python
# 这里是执行随机 Pandas DataFrame 操作的一个例子，限制行数。
df_query = "从第 1 行到第 3 行检索 num_legs 列的平均值。"
# 设置提示。
prompt = PromptTemplate(
    template="回答用户查询。\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)
chain = prompt | model | parser
parser_output = chain.invoke({"query": df_query})
print(parser_output)
```

```python
# 这里是执行格式不正确的查询的一个例子。
df_query = "检索 num_fingers 列的平均值。"
# 设置提示。
prompt = PromptTemplate(
    template="回答用户查询。\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)
chain = prompt | model | parser
parser_output = chain.invoke({"query": df_query})
```

## 9、Pydantic parser

这个输出解析器允许用户指定任意的 Pydantic 模型，并查询 LLM 以获取符合该模式的输出。

请记住，大型语言模型是泄露的抽象！您必须使用具有足够容量的 LLM 来生成构造良好的 JSON。在 OpenAI 系列中，DaVinci 可以可靠地做到这一点，但 Curie 的能力已经大幅下降。

使用 Pydantic 声明您的数据模型。Pydantic 的 BaseModel 就像 Python 的数据类，但具有实际的类型检查 + 强制转换。

```python
from typing import List

from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field, validator
from langchain_openai import ChatOpenAI
model = ChatOpenAI(temperature=0)

# Define your desired data structure.
class Joke(BaseModel):
    setup: str = Field(description="question to set up a joke")
    punchline: str = Field(description="answer to resolve the joke")

    # You can add custom validation logic easily with Pydantic.
    @validator("setup")
    def question_ends_with_question_mark(cls, field):
        if field[-1] != "?":
            raise ValueError("Badly formed question!")
        return field


# And a query intented to prompt a language model to populate the data structure.
joke_query = "Tell me a joke."

# Set up a parser + inject instructions into the prompt template.
parser = PydanticOutputParser(pydantic_object=Joke)

prompt = PromptTemplate(
    template="Answer the user query.\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)

chain = prompt | model | parser

chain.invoke({"query": joke_query})
```

```python
setup="Why couldn't the bicycle find its way home?" punchline='Because it lost its bearings!'
```

```python
# Here's another example, but with a compound typed field.
class Actor(BaseModel):
    name: str = Field(description="name of an actor")
    film_names: List[str] = Field(description="list of names of films they starred in")


actor_query = "Generate the filmography for a random actor."

parser = PydanticOutputParser(pydantic_object=Actor)

prompt = PromptTemplate(
    template="Answer the user query.\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)

chain = prompt | model | parser

chain.invoke({"query": actor_query})
```

```text
name='Tom Hanks' film_names=['Forrest Gump', 'Cast Away', 'Saving Private Ryan', 'Toy Story', 'The Green Mile', 'Apollo 13', 'Philadelphia', 'Captain Phillips', 'Sully', 'The Da Vinci Code']
```

## 10、Retry parser

虽然在某些情况下，仅通过查看输出就能修复任何解析错误，但在其他情况下则不行。一个例子是，输出不仅格式不正确，而且是部分完成的。考虑以下示例。

```python
from langchain.output_parsers import (
    OutputFixingParser,
    PydanticOutputParser,
)
from langchain.prompts import (
    PromptTemplate,
)
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI, OpenAI

template = """
根据用户问题，提供应该采取的行动和行动输入。
{format_instructions}
问题：{query}
响应：
"""


class Action(BaseModel):
    action: str = Field(description="要采取的行动")
    action_input: str = Field(description="行动的输入")


parser = PydanticOutputParser(pydantic_object=Action)
prompt = PromptTemplate(
    template="回答用户查询。\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)
prompt_value = prompt.format_prompt(query="谁是莱昂纳多·迪卡普里奥的女朋友？")
```

```python
bad_response = '{"action": "search"}'
```

如果我们尝试按原样解析这个响应，我们会得到一个错误：

```python
parser.parse(bad_response)
```

```python
OutputParserException: Failed to parse Action from completion {"action": "search"}. Got: 1 validation error for Action
action_input
  field required (type=value_error.missing)
```

如果我们尝试使用 `OutputFixingParser` 来修复这个错误，它会感到困惑 - 也就是说，它不知道实际上应该为行动输入放什么。

```python
fix_parser = OutputFixingParser.from_llm(parser=parser, llm=ChatOpenAI())
```

```python
fix_parser.parse(bad_response)
```

```python
Action(action='search', action_input='input')
```

相反，我们可以使用 RetryOutputParser，它传递提示（以及原始输出）再次尝试以获得更好的响应。

```python
from langchain.output_parsers import RetryOutputParser
```

```python
retry_parser = RetryOutputParser.from_llm(parser=parser, llm=OpenAI(temperature=0))
```

```python
retry_parser.parse_with_prompt(bad_response, prompt_value)
```

```python
Action(action='search', action_input='莱昂纳多·迪卡普里奥的女朋友')
```

我们也可以轻松地将 RetryOutputParser 添加到自定义链中，该链将原始的 LLM/聊天模型输出转换为更易处理的格式。

```python
from langchain_core.runnables import RunnableLambda, RunnableParallel
completion_chain = prompt | OpenAI(temperature=0)
main_chain = RunnableParallel(
    completion=completion_chain,
    prompt_value=prompt,
) | RunnableLambda(lambda x: retry_parser.parse_with_prompt(**x))
main_chain.invoke({"query": "谁是莱昂纳多·迪卡普里奥的女朋友？"})
```

```python
Action(action='search', action_input='莱昂纳多·迪卡普里奥的女朋友')
```

## 11、Structured output parser

这个输出解析器可以在您想要返回多个字段时使用。虽然 Pydantic/JSON 解析器更强大，但对于功能较弱的模型来说，这个解析器很有用。

```python
from langchain.output_parsers import ResponseSchema, StructuredOutputParser
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
```

```python
response_schemas = [
    ResponseSchema(name="answer", description="用户问题的答案"),
    ResponseSchema(
        name="source",
        description="回答问题的来源，应该是一个网站。",
    ),
]
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
```

现在我们得到了一个包含如何格式化响应的指令字符串，然后我们将这个插入到我们的提示中。

```python
from langchain.output_parsers import ResponseSchema, StructuredOutputParser
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

response_schemas = [
    ResponseSchema(name="answer", description="用户问题的答案"),
    ResponseSchema(
        name="source",
        description="回答问题的来源，应该是一个网站。",
    ),
]
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)
format_instructions = output_parser.get_format_instructions()
prompt = PromptTemplate(
    template="尽可能好地回答用户的问题。\n{format_instructions}\n{question}",
    input_variables=["question"],
    partial_variables={"format_instructions": format_instructions},
)
model = ChatOpenAI(temperature=0)
chain = prompt | model | output_parser
print(chain.invoke({"question": "法国的首都是哪里？"}))
```

```python
{'answer': '法国的首都是巴黎。', 'source': 'https://en.wikipedia.org/wiki/Paris'}
```

```python
for s in chain.stream({"question": "法国的首都是哪里？"}):
    print(s)
```

```python
{'answer': '法国的首都是巴黎。', 'source': 'https://en.wikipedia.org/wiki/Paris'}
```

## 12、XML parser

这个输出解析器允许用户以流行的 XML 格式从大型语言模型（LLM）获取结果。请记住，大型语言模型是具有泄露性的抽象！您需要使用具有足够容量的 LLM 来生成格式良好的 XML。

在以下示例中，我们使用了 Claude 模型（[https://docs.anthropic.com/claude/docs](https://docs.anthropic.com/claude/docs)），该模型与 XML 标签配合得非常好。

```python
from langchain.output_parsers import XMLOutputParser
from langchain.prompts import PromptTemplate
from langchain_community.chat_models import ChatAnthropic
```

```python
model = ChatAnthropic(model="claude-2", max_tokens_to_sample=512, temperature=0.1)
```

让我们从对模型的简单请求开始。

```python
actor_query = "Generate the shortened filmography for Tom Hanks."
output = model.invoke(
    f"""{actor_query}Please enclose the movies in <movie></movie> tags"""
)
print(output.content)
```

这里是 Tom Hanks 的简短电影作品列表，用 XML 标签包围：

```xml
<movie>Splash</movie>
<movie>Big</movie>
<movie>A League of Their Own</movie>
<movie>Sleepless in Seattle</movie>
<movie>Forrest Gump</movie>
<movie>Toy Story</movie>
<movie>Apollo 13</movie>
<movie>Saving Private Ryan</movie>
<movie>Cast Away</movie>
<movie>The Da Vinci Code</movie>
<movie>Captain Phillips</movie>
```

现在我们将使用 XMLOutputParser 来获得结构化输出。

```python
parser = XMLOutputParser()
prompt = PromptTemplate(
    template="\"\"\{query}\n{format_instructions}\"\"\"",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)
chain = prompt | model | parser
output = chain.invoke({"query": actor_query})
print(output)
```

```python
{
    'filmography': [
        {'movie': [{'title': 'Big'}, {'year': '1988'}]},
        {'movie': [{'title': 'Forrest Gump'}, {'year': '1994'}]},
        {'movie': [{'title': 'Toy Story'}, {'year': '1995'}]},
        {'movie': [{'title': 'Saving Private Ryan'}, {'year': '1998'}]},
        {'movie': [{'title': 'Cast Away'}, {'year': '2000'}]}
    ]
}
```

最后，让我们添加一些标签来定制我们所需的输出。

```python
parser = XMLOutputParser(tags=["movies", "actor", "film", "name", "genre"])
prompt = PromptTemplate(
    template="\"\"\{query}\n{format_instructions}\"\"\"",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)
chain = prompt | model | parser
output = chain.invoke({"query": actor_query})
print(output)
```

```python
{
    'movies': [
        {
            'actor': [
                {
                    'name': 'Tom Hanks',
                    'film': [
                        {'name': 'Forrest Gump', 'genre': 'Drama'},
                        {'name': 'Cast Away', 'genre': 'Adventure'},
                        {'name': 'Saving Private Ryan', 'genre': 'War'}
                    ]
                }
            ]
        }
    ]
}
```

```python
for s in chain.stream({"query": actor_query}):
    print(s)
```

```python
{
    'movies': [{'actor': [{'name': 'Tom Hanks'}]}]
    'movies': [{'actor': {'film': [{'name': 'Forrest Gump'}]}}]
    'movies': [{'actor': {'film': [{'genre': 'Drama'}]}}]
    'movies': [{'actor': {'film': [{'name': 'Cast Away'}]}}]
    'movies': [{'actor': {'film': [{'genre': 'Adventure'}]}}]
    'movies': [{'actor': {'film': [{'name': 'Saving Private Ryan'}]}}]
    'movies': [{'actor': {'film': [{'genre': 'War'}]}}]
}
```

## 13、YAML parser

这个输出解析器允许用户指定任意的模式，并查询大型语言模型（LLM）以获取符合该模式的输出，使用 YAML 来格式化他们的回答。

请记住，大型语言模型是具有泄露性的抽象！您需要使用具有足够容量的 LLM 来生成格式良好的 YAML。在 OpenAI 家族中，DaVinci 可以可靠地做到这一点，但 Curie 的能力已经显著下降。

您可以选择使用 Pydantic 来声明您的数据模型。

```python
from langchain.output_parsers import YAMLOutputParser
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
```

```python
output_parser = YAMLOutputParser()
prompt = PromptTemplate(
    template="Respond with the following YAML schema:\n\n```yaml\njoke:\n  setup: \"Why don't scientists trust atoms?\"\n  punchline: 'Because they make up everything!'\n```",
)
chain = prompt | model | output_parser
```

```python
chain.invoke()
```

```yaml
joke:
  setup: "Why don't scientists trust atoms?"
  punchline: "Because they make up everything!"
```

```python
for s in chain.stream():
    print(s)
```

```yaml
joke:
  setup: "Why don't scientists trust atoms?"
  punchline: "Because they make up everything!"
```
