在本指南中，我们将学习如何使用LangChain抽象创建自定义聊天模型。

将您的LLM（大型语言模型）包装在标准的`ChatModel`接口中，可以让您在现有的LangChain程序中使用LLM，同时只需进行最小的代码修改！

作为额外好处，您的LLM将自动成为LangChain的`Runnable`，并且将立即受益于一些开箱即用的优化（例如，通过线程池批量处理），异步支持，`astream_events` API等。

## 1、输入和输出

首先，我们需要讨论消息，它们是聊天模型的输入和输出。

### （1）消息

聊天模型接受消息作为输入，并返回一个消息作为输出。

LangChain有几个内置的消息类型：

- `SystemMessage`：用于引导AI行为，通常作为输入消息序列的第一个传递。
- `HumanMessage`：代表与人交互的聊天模型的消息。
- `AIMessage`：代表来自聊天模型的消息。这可以是文本或调用工具的请求。
- `FunctionMessage` / `ToolMessage`：用于将工具调用的结果传递回模型的消息。

**注意：**`ToolMessage`和`FunctionMessage`紧密遵循OpenAI的`function`和`tool`参数。

这是一个快速发展的领域，随着更多模型添加函数调用功能，预计会对此模式进行补充。

```python
from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    FunctionMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
)
```

### （2）流变体

所有聊天消息都有一个包含`Chunk`名称的流变体。

```python
from langchain_core.messages import (
    AIMessageChunk,
    FunctionMessageChunk,
    HumanMessageChunk,
    SystemMessageChunk,
    ToolMessageChunk,
)
```

这些块在从聊天模型流式传输输出时使用，它们都定义了一个加法属性！

```python
AIMessageChunk(content="Hello") + AIMessageChunk(content=" World!")
```

```python
AIMessageChunk(content='Hello World!')
```

## 2、简单聊天模型

从`SimpleChatModel`继承非常适合原型设计！

它不允许您实现所有您可能想要的聊天模型功能，但它快速实现，如果您需要更多，可以过渡到下面显示的`BaseChatModel`。

让我们实现一个回显提示最后`n`个字符的聊天模型！

您需要实现以下内容：

- 方法`_call` - 用于从提示生成聊天结果。

此外，您可以选择指定以下内容：

- 属性`_identifying_params` - 用于记录目的的模型参数化。

可选：

- `_stream` - 用于实现流式传输。

## 3、基础聊天模型

让我们实现一个回显提示中最后一条消息的前`n`个字符的聊天模型！

为此，我们将从`BaseChatModel`继承，我们需要实现以下方法/属性：

此外，您可以选择指定以下内容：

从`BaseChatModel`继承，这是一个较低级别的类，并实现以下方法：

- `_generate` - 用于从提示生成聊天结果
- 属性`_llm_type` - 用于唯一标识模型类型。用于记录。
- 可选：
  - `_stream` - 用于实现流式传输。
  - `_agenerate` - 用于实现原生异步方法。
  - `_astream` - 用于实现`_stream`的异步版本。
  - 属性`_identifying_params` - 用于记录目的的模型参数化。

!>**注意：**目前，要使异步流式传输（通过`astream`）工作，您必须提供`_astream`的实现。如果未提供`_astream`，则异步流式传输将回退到`_agenerate`，这不支持逐个令牌流式传输。

### （1）实现

```python
from typing import Any, AsyncIterator, Dict, Iterator, List, Optional
from langchain_core.callbacks import (
    AsyncCallbackManagerForLLMRun,
    CallbackManagerForLLMRun,
)
from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    FunctionMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
)
from langchain_core.language_models import BaseChatModel, SimpleChatModel
from langchain_core.messages import AIMessageChunk, BaseMessage, HumanMessage
from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult
from langchain_core.runnables import run_in_executor


class CustomChatModelAdvanced(BaseChatModel):
    """一个自定义聊天模型，回显输入的前`n`个字符。
    当向LangChain贡献实现时，仔细记录模型，包括初始化参数，
    包括如何初始化模型的示例，并包括任何相关的底层模型文档或API的链接。
    Example:
        model = CustomChatModel(n=2)
        result = model.invoke([HumanMessage(content="hello")])
        result = model.batch([[HumanMessage(content="hello")], [HumanMessage(content="world")]])
    """

    n: int
    """从提示的最后一条消息中回显的字符数。"""

    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """重写_generate方法以实现聊天模型逻辑。
        这可以是对API的调用，对本地模型的调用，或任何其他实现对输入提示的响应的实现。
        Args:
            messages: 由消息列表组成的提示。
            stop: 模型应该停止生成的字符串列表。
                如果由于停止令牌而停止生成，停止令牌本身应该包含在输出中。
               目前不是所有模型都强制执行此操作，但遵循这是一个好习惯，因为它使得在下游更容易解析模型的输出，并理解为什么停止生成。
            run_manager: 具有LLM回调的运行管理器。
        """
        last_message = messages[-1]
        tokens = last_message.content[: self.n]
        message = AIMessage(content=tokens)
        generation = ChatGeneration(message=message)
        return ChatResult(generations=[generation])

    def _stream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[ChatGenerationChunk]:
        """流式传输模型的输出。
        如果模型可以以流式传输方式生成输出，则应实现此方法。如果模型不支持流式传输，请不要实现它。在这种情况下，流式传输请求将由_generate方法自动处理。
        Args:
            messages: 由消息列表组成的提示。
            stop: 模型应该停止生成的字符串列表。
                如果由于停止令牌而停止生成，停止令牌本身应该包含在输出中。
               目前不是所有模型都强制执行此操作，但遵循这是一个好习惯，因为它使得在下游更容易解析模型的输出，并理解为什么停止生成。
            run_manager: 具有LLM回调的运行管理器。
        """
        last_message = messages[-1]
        tokens = last_message.content[: self.n]
        for token in tokens:
            chunk = ChatGenerationChunk(message=AIMessageChunk(content=token))
            if run_manager:
                run_manager.on_llm_new_token(token, chunk=chunk)
            yield chunk

    async def _astream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> AsyncIterator[ChatGenerationChunk]:
        """astream的异步变体。
        如果未提供，则默认行为是委托给_generate方法。
        下面的实现将委托给_stream，并将其实现在单独的线程中启动。
        如果您能够本地支持异步，则一定要这样做！
        """
        result = await run_in_executor(
            None,
            self._stream,
            messages,
            stop=stop,
            run_manager=run_manager.get_sync() if run_manager else None,
            **kwargs,
        )
        for chunk in result:
            yield chunk

    @property
    def _llm_type(self) -> str:
        """获取此聊天模型使用的语音模型类型。"""
        return "echoing-chat-model-advanced"

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """返回一个标识参数的字典。"""
        return {"n": self.n}

```

!>`_astream`实现使用`run_in_executor`在单独的线程中启动同步`_stream`。

如果您想重用`_stream`实现，可以使用这个技巧，但如果您能够实现本地异步代码，那将是更好的解决方案，因为这样的代码将具有更少的开销。

### （2）测试

聊天模型将实现LangChain的标准`Runnable`接口，许多LangChain抽象都支持它！

```python
model = CustomChatModelAdvanced(n=3)
```

```python
model.invoke(
    [
        HumanMessage(content="hello!"),
        AIMessage(content="Hi there human!"),
        HumanMessage(content="Meow!"),
    ]
)
```

```python
model.batch(["hello", "goodbye"])
```

```text
[AIMessage(content='hel'), AIMessage(content='goo')]
```

```python
for chunk in model.stream("cat"):
    print(chunk.content, end="|")
```

请查看模型中的`_astream`实现！如果您没有实现它，那么将不会有输出流。！

```python
async for chunk in model.astream("cat"):
    print(chunk.content, end="|")
```

让我们尝试使用astream事件API，这也将帮助我们再次确认所有回调都已实现！

```python
async for event in model.astream_events("cat", version="v1"):
    print(event)
```

```
{'event': 'on_chat_model_start', 'run_id': 'e03c0b21-521f-4cb4-a837-02fed65cf1cf', 'name': 'CustomChatModelAdvanced', 'tags': [], 'metadata': {}, 'data': {'input': 'cat'}}
{'event': 'on_chat_model_stream', 'run_id': 'e03c0b21-521f-4cb4-a837-02fed65cf1cf', 'tags': [], 'metadata': {}, 'name': 'CustomChatModelAdvanced', 'data': {'chunk': AIMessageChunk(content='c')}}
{'event': 'on_chat_model_stream', 'run_id': 'e03c0b21-521f-4cb4-a837-02fed65cf1cf', 'tags': [], 'metadata': {}, 'name': 'CustomChatModelAdvanced', 'data': {'chunk': AIMessageChunk(content='a')}}
{'event': 'on_chat_model_stream', 'run_id': 'e03c0b21-521f-4cb4-a837-02fed65cf1cf', 'tags': [], 'metadata': {}, 'name': 'CustomChatModelAdvanced', 'data': {'chunk': AIMessageChunk(content='t')}}
{'event': 'on_chat_model_end', 'name': 'CustomChatModelAdvanced', 'run_id': 'e03c0b21-521f-4cb4-a837-02fed65cf1cf', 'tags': [], 'metadata': {}, 'data': {'output': AIMessageChunk(content='cat')}}
```

## 4、标识参数

LangChain具有回调系统，允许实现日志记录器来监控LLM应用程序的行为。

还记得之前的`_identifying_params`属性吗？

它被传递给回调系统，并且对于用户指定的日志记录器是可访问的。

下面我们将实现一个处理程序，只用一个`on_chat_model_start`事件来看看`_identifying_params`在哪里出现。

```python
from uuid import UUID
from langchain_core.callbacks import AsyncCallbackHandler
from langchain_core.outputs import (
    ChatGenerationChunk,
    ChatResult,
    GenerationChunk,
    LLMResult,
)
class SampleCallbackHandler(AsyncCallbackHandler):
    """异步回调处理程序，处理来自LangChain的回调。"""
    async def on_chat_model_start(
        self,
        serialized: Dict[str, Any],
        messages: List[List[BaseMessage]],
        *,
        run_id: UUID,
        parent_run_id: Optional[UUID] = None,
        tags: Optional[List[str]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        **kwargs: Any,
    ) -> Any:
        """当聊天模型开始运行时运行。"""
        print("---")
        print("On chat model start.")
        print(kwargs)
model.invoke("meow", stop=["woof"], config={"callbacks": [SampleCallbackHandler()]})
```

```
---
On chat model start.
{'invocation_params': {'n': 3, '_type': 'echoing-chat-model-advanced', 'stop': ['woof']}, 'options': {'stop': ['woof']}, 'name': None, 'batch_size': 1}
content='meo'
```
