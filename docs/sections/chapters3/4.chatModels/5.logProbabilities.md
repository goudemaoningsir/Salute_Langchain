> 某些聊天模型可以配置为返回token级别的日志概率。本指南将介绍如何获取多个模型的对数概率。

为了使OpenAI API返回日志概率，我们需要配置参数 `logprobs=True`。

```python
from langchain_openai import ChatOpenAI, OpenAI

print("####################      step1 初始化模型     ####################")
chat_model = ChatOpenAI(model="gpt-3.5-turbo").bind(logprobs=True)

print("####################      step2 调用模型       ####################")
msg = chat_model.invoke(("human", "how are you today"))
print(msg)
print(msg.response_metadata["logprobs"]["content"][:5])
```

对数概率包含在每个输出消息的 `response_metadata` 中：

```text
[{'token': 'I', 'bytes': [73], 'logprob': -0.21393852, 'top_logprobs': []}, {'token': "'m", 'bytes': [39, 109], 'logprob': -0.37652057, 'top_logprobs': []}, {'token': ' just', 'bytes': [32, 106, 117, 115, 116], 'logprob': -0.20173045, 'top_logprobs': []}, {'token': ' a', 'bytes': [32, 97], 'logprob': -0.0019989389, 'top_logprobs': []}, {'token': ' computer', 'bytes': [32, 99, 111, 109, 112, 117, 116, 101, 114], 'logprob': -0.05375806, 'top_logprobs': []}]
```

并且也是流式消息块的一部分：

```python
ct = 0
full = None
for chunk in llm.stream(("human", "how are you today")):
    if ct < 5:
        full = chunk if full is None else full + chunk
        if "logprobs" in full.response_metadata:
            print(full.response_metadata["logprobs"]["content"])
    else:
        break
    ct += 1
```

```text
[{'token': 'I', 'bytes': [73], 'logprob': -0.22074652, 'top_logprobs': []}, {'token': ' appreciate', 'bytes': [32, 97, 112, 112, 114, 101, 99, 105, 97, 116, 101], 'logprob': -2.2043924, 'top_logprobs': []}, {'token': ' your', 'bytes': [32, 121, 111, 117, 114], 'logprob': -0.05850012, 'top_logprobs': []}, {'token': ' concern', 'bytes': [32, 99, 111, 110, 99, 101, 114, 110], 'logprob': -0.6564238, 'top_logprobs': []}]
```

请注意，上述内容中的 `logprob` 字段代表的是每个token的对数概率值，这是一个衡量token在语言模型中出现概率的指标。对数概率通常用于自然语言处理中的各种任务，例如语言模型评估、文本生成和机器翻译等。在LangChain中，可以通过配置聊天模型来获取这些对数概率值，以便在后续的处理中使用。