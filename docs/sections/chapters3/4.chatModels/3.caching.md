LangChain为聊天模型提供了一个可选的缓存层。这有两个用途：

- 如果你经常多次请求相同的完成（completion），它可以减少你向LLM提供商发出的API调用数量，从而节省费用。
- 通过减少向LLM提供商发出的API调用数量，它可以加快你的应用程序的速度。

```python
# 📌 注意：此处忽略F821编码错误
from langchain.globals import set_llm_cache
```

## 1、内存缓存

```python
from langchain_openai import ChatOpenAI

print("####################      step0 计算函数执行时间     ####################")
import time


def timing_decorator(func):
    def wrapper(*args, **kwargs):
        start_time = time.time()  # 记录函数开始执行的时间
        result = func(*args, **kwargs)  # 执行被装饰的函数
        end_time = time.time()  # 记录函数结束执行的时间
        elapsed_time = end_time - start_time  # 计算执行时间
        print(f"{func.__name__} took {elapsed_time} seconds to execute.")
        return result

    return wrapper


print("####################      step1 初始化模型     ####################")
chat = ChatOpenAI(model="gpt-3.5-turbo")

print("####################      step2  内存缓存​     ####################")
from langchain.globals import set_llm_cache
from langchain.cache import InMemoryCache

set_llm_cache(InMemoryCache())
# 第一次，它尚未缓存，所以应该需要更长时间


@timing_decorator
def predict_and_time(chat_instance, text):
    return chat_instance.invoke(text)


print(predict_and_time(chat, "Tell me a joke"))
```

```plaintext
predict_and_time took 2.010190010070801 seconds to execute.
```

```plaintext
content='Why did the scarecrow win an award?\n\nBecause he was outstanding in his field!' response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 11, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}
```

```python
# 第二次，它已经在缓存中，所以会更快
print(predict_and_time(chat, "Tell me a joke"))
```

```plaintext
predict_and_time took 0.001024484634399414 seconds to execute.
```

```plaintext
content='Why did the scarecrow win an award?\n\nBecause he was outstanding in his field!' response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 11, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}
```

## 2、SQLite 缓存

我们可以使用SQLite缓存做同样的事情

```python
from langchain.cache import SQLiteCache
set_llm_cache(SQLiteCache(database_path=".langchain.db"))
```

```python
print(predict_and_time(chat, "Tell me a joke"))
```

```plaintext
predict_and_time took 1.255873203277588 seconds to execute.
```

```plaintext
content='Why did the scarecrow win an award?\n\nBecause he was outstanding in his field!' response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 11, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}
```
第二次，它已经在缓存中，所以会更快
```python
print(predict_and_time(chat, "Tell me a joke"))
```

```plaintext
predict_and_time took 0.04606485366821289 seconds to execute.
```

```plaintext
content='Why did the scarecrow win an award?\n\nBecause he was outstanding in his field!'
```
