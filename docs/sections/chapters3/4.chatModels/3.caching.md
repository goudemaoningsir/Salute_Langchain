LangChainä¸ºèŠå¤©æ¨¡å‹æä¾›äº†ä¸€ä¸ªå¯é€‰çš„ç¼“å­˜å±‚ã€‚è¿™æœ‰ä¸¤ä¸ªç”¨é€”ï¼š

- å¦‚æœä½ ç»å¸¸å¤šæ¬¡è¯·æ±‚ç›¸åŒçš„å®Œæˆï¼ˆcompletionï¼‰ï¼Œå®ƒå¯ä»¥å‡å°‘ä½ å‘LLMæä¾›å•†å‘å‡ºçš„APIè°ƒç”¨æ•°é‡ï¼Œä»è€ŒèŠ‚çœè´¹ç”¨ã€‚
- é€šè¿‡å‡å°‘å‘LLMæä¾›å•†å‘å‡ºçš„APIè°ƒç”¨æ•°é‡ï¼Œå®ƒå¯ä»¥åŠ å¿«ä½ çš„åº”ç”¨ç¨‹åºçš„é€Ÿåº¦ã€‚

```python
# ğŸ“Œ æ³¨æ„ï¼šæ­¤å¤„å¿½ç•¥F821ç¼–ç é”™è¯¯
from langchain.globals import set_llm_cache
```

## 1ã€å†…å­˜ç¼“å­˜

```python
from langchain_openai import ChatOpenAI

print("####################      step0 è®¡ç®—å‡½æ•°æ‰§è¡Œæ—¶é—´     ####################")
import time


def timing_decorator(func):
    def wrapper(*args, **kwargs):
        start_time = time.time()  # è®°å½•å‡½æ•°å¼€å§‹æ‰§è¡Œçš„æ—¶é—´
        result = func(*args, **kwargs)  # æ‰§è¡Œè¢«è£…é¥°çš„å‡½æ•°
        end_time = time.time()  # è®°å½•å‡½æ•°ç»“æŸæ‰§è¡Œçš„æ—¶é—´
        elapsed_time = end_time - start_time  # è®¡ç®—æ‰§è¡Œæ—¶é—´
        print(f"{func.__name__} took {elapsed_time} seconds to execute.")
        return result

    return wrapper


print("####################      step1 åˆå§‹åŒ–æ¨¡å‹     ####################")
chat = ChatOpenAI(model="gpt-3.5-turbo")

print("####################      step2  å†…å­˜ç¼“å­˜â€‹     ####################")
from langchain.globals import set_llm_cache
from langchain.cache import InMemoryCache

set_llm_cache(InMemoryCache())
# ç¬¬ä¸€æ¬¡ï¼Œå®ƒå°šæœªç¼“å­˜ï¼Œæ‰€ä»¥åº”è¯¥éœ€è¦æ›´é•¿æ—¶é—´


@timing_decorator
def predict_and_time(chat_instance, text):
    return chat_instance.invoke(text)


print(predict_and_time(chat, "Tell me a joke"))
```

```plaintext
predict_and_time took 2.010190010070801 seconds to execute.
```

```plaintext
content='Why did the scarecrow win an award?\n\nBecause he was outstanding in his field!' response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 11, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}
```

```python
# ç¬¬äºŒæ¬¡ï¼Œå®ƒå·²ç»åœ¨ç¼“å­˜ä¸­ï¼Œæ‰€ä»¥ä¼šæ›´å¿«
print(predict_and_time(chat, "Tell me a joke"))
```

```plaintext
predict_and_time took 0.001024484634399414 seconds to execute.
```

```plaintext
content='Why did the scarecrow win an award?\n\nBecause he was outstanding in his field!' response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 11, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}
```

## 2ã€SQLite ç¼“å­˜

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨SQLiteç¼“å­˜åšåŒæ ·çš„äº‹æƒ…

```python
from langchain.cache import SQLiteCache
set_llm_cache(SQLiteCache(database_path=".langchain.db"))
```

```python
print(predict_and_time(chat, "Tell me a joke"))
```

```plaintext
predict_and_time took 1.255873203277588 seconds to execute.
```

```plaintext
content='Why did the scarecrow win an award?\n\nBecause he was outstanding in his field!' response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 11, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_b28b39ffa8', 'finish_reason': 'stop', 'logprobs': None}
```
ç¬¬äºŒæ¬¡ï¼Œå®ƒå·²ç»åœ¨ç¼“å­˜ä¸­ï¼Œæ‰€ä»¥ä¼šæ›´å¿«
```python
print(predict_and_time(chat, "Tell me a joke"))
```

```plaintext
predict_and_time took 0.04606485366821289 seconds to execute.
```

```plaintext
content='Why did the scarecrow win an award?\n\nBecause he was outstanding in his field!'
```
