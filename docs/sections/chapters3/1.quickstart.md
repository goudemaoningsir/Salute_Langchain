## 1、安装LangChain库

要开始使用LangChain进行语言模型开发，首先需要安装合作伙伴的包。对于使用Anthropic或OpenAI的API，或者是通过Ollama使用本地开源模型，您可以通过执行以下命令安装所需的包：

```bash
pip install langchain-openai
```

## 2、获取API密钥

要访问API，首先需要获取一个API密钥。这可以通过创建账户并访问特定的网址来完成。获取API密钥后，通过以下命令将其设置为环境变量，以便在应用程序中使用：

```bash
export OPENAI_API_KEY="..."
```

## 3、初始化模型

安装包并获取API密钥后，下一步是初始化模型。下面的代码展示了如何初始化大型语言模型（LLM）和聊天模型（ChatModel）：

```python
from langchain_openai import ChatOpenAI, OpenAI
llm = OpenAI()
chat_model = ChatOpenAI(model="gpt-3.5-turbo")
```

如果您选择不设置环境变量，也可以在初始化模型时直接通过参数传递API密钥：

```python
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(openai_api_key="...")
```

## 4、调用模型

初始化模型后，可以根据需要调用模型。以下示例演示了如何分别调用LLM和ChatModel，并展示了它们处理输入和输出的不同方式：

```python
from langchain_core.messages import HumanMessage
text = "为一家生产彩色袜子的公司起一个好名字是什么？"
messages = [HumanMessage(content=text)]
llm.invoke(text)  # Rainbow Threads
chat_model.invoke(messages)  # content='“彩虹袜子公司”' response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 30, 'total_tokens': 40}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None}
```

LLM返回一个字符串，而ChatModel返回一个消息。

## 5、使用提示模板

在大多数应用场景中，直接将用户输入传递给模型并不常见。相反，一般会将用户的输入放入一个更大的文本框架中——即提示模板，为特定任务提供额外的上下文：

```python
from langchain.prompts import PromptTemplate
prompt = PromptTemplate.from_template("一家生产{product}的公司的好名字是什么？")
prompt.format(product="彩色袜子") # 一家生产彩色袜子的公司的好名字是什么？
```

在使用LangChain库构建聊天模型时，`PromptTemplate`不仅可用于生成单一文本提示，还能生成包含多个消息的列表。这在构建对话式AI应用时尤其有用，因为它允许每个消息带有额外的信息，如消息的角色（例如是由系统还是用户生成）和消息在对话中的位置。

`ChatPromptTemplate`是一个特殊类型的模板，专门用于生成这样的消息列表。它由一系列`ChatMessageTemplate`组成，每个`ChatMessageTemplate`定义了如何格式化一个聊天消息，包括消息的角色和内容。

以下示例演示了如何使用`ChatPromptTemplate`来创建一个提示，该提示模拟一个能够将一种语言翻译成另一种语言的助手。这个例子中，模板包括一个系统消息和一个用户消息：

1. **系统消息**定义了一个场景，即助手的角色是帮助用户从一种语言翻译到另一种语言。
2. **用户消息**则是要翻译的文本。

通过调用`ChatPromptTemplate`的`from_messages`方法，我们创建了一个聊天提示模板，并通过`format_messages`方法填充具体的语言和文本内容：

```python
from langchain.prompts.chat import ChatPromptTemplate
template = "You are a helpful assistant that translates {input_language} to {output_language}."
human_template = "{text}"
chat_prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    ("human", human_template),
])
out = chat_prompt.format_messages(input_language="English", output_language="French", text="I love programming.")
print(out) # [SystemMessage(content='You are a helpful assistant that translates English to French.'), HumanMessage(content='I love programming.')]
```

在这个过程中，`chat_prompt.format_messages`方法根据提供的输入语言（英语）、输出语言（法语）和用户文本（"I love programming."）生成了一系列格式化的消息。这些消息既可以直接用于对话系统的输入，也可以作为对话管理的一部分，以进一步处理和响应。

## 6、输出解析

最后，输出解析器将模型的原始输出转换成应用程序可以直接使用的格式。以下是使用输出解析器的一个例子：

```python
from langchain.output_parsers import CommaSeparatedListOutputParser
output_parser = CommaSeparatedListOutputParser()
output_parser.parse("你好, 再见")  # 返回 ['你好', '再见']
```

这样，通过将输入、提示模板、模型和输出解析器结合起来，我们可以构建一个完整的处理流程，有效地将复杂的逻辑封装成模块化的链条，从而提高开发效率和灵活性。

## 7、组合链

我们现在可以将所有这些组合成一个链。这个链将接受输入变量，将这些变量传递给提示模板以创建提示，将提示传递给语言模型，然后通过（可选的）输出解析器传递输出。这是一种方便的方式来打包一个模块化的逻辑。

```python
template = "Generate a list of 5 {text}.\n\n{format_instructions}"

chat_prompt = ChatPromptTemplate.from_template(template)
chat_prompt = chat_prompt.partial(format_instructions=output_parser.get_format_instructions())
chain = chat_prompt | chat_model | output_parser
chain.invoke({"text": "colors"})
# >> ['red', 'blue', 'green', 'yellow', 'orange']
```

注意，我们使用`|`语法将这些组件连接在一起。这个`|`语法由LangChain表达式语言（LCEL）提供支持，并依赖于所有这些对象实现的通用`Runnable`接口。