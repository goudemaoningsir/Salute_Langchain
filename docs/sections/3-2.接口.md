为了尽可能简化创建自定义链的过程，我们实现了一个 ["Runnable"](https://api.python.langchain.com/en/stable/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable) 协议。`Runnable` 协议已为大多数组件实现。 这是一个标准接口，可以轻松定义自定义链并以标准方式调用它们。 标准接口包括：

- `stream`: 流式返回响应的块
- `invoke`: 在输入上调用链
- `batch`: 在输入列表上调用链

这些方法也有对应的异步方法:

- `astream`: 异步流式返回响应的块
- `ainvoke`: 异步在输入上调用链
- `abatch`: 异步在输入列表上调用链
- `astream_log`: 异步流式返回中间步骤，以及最终响应
- `astream_events`: **beta** 异步流式返回链中发生的事件（在 `langchain-core` 0.1.14 中引入）

**输入类型**和**输出类型**因组件而异:

| 组件         | 输入类型                               | 输出类型     |
| ------------ | -------------------------------------- | ------------ |
| Prompt       | 字典                                   | PromptValue  |
| ChatModel    | 单个字符串、聊天消息列表或 PromptValue | ChatMessage  |
| LLM          | 单个字符串、聊天消息列表或 PromptValue | 字符串       |
| OutputParser | LLM 或 ChatModel 的输出                | 取决于解析器 |
| Retriever    | 单个字符串                             | 文档列表     |
| Tool         | 单个字符串或字典，取决于工具           | 取决于工具   |

所有可运行对象都公开输入和输出的**模式**以检查输入和输出:

- `input_schema`: 从 Runnable 的结构动态生成的输入 Pydantic 模型
- `output_schema`: 从 Runnable 的结构动态生成的输出 Pydantic 模型

让我们来看看这些方法。为此，我们将创建一个超级简单的 PromptTemplate + ChatModel 链。

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
chain = prompt | model
```

## 1、输入模式

Runnable 接受的输入的描述。 这是从任何 Runnable 的结构动态生成的 Pydantic 模型。 您可以调用 `.schema()` 来获取其 JSONSchema 表示形式。

```python
# 链的输入模式是其第一个部分（prompt）的输入模式。
chain.input_schema.schema()
prompt.input_schema.schema()
model.input_schema.schema()
```

## 2、输出模式

对由可运行对象产生的输出的描述。 这是根据任何可运行对象的结构动态生成的 Pydantic 模型。 您可以调用 `.schema()` 来获取 JSONSchema 表示形式。

```python
# 链的输出模式是其最后一部分的输出模式，本例中是 ChatModel，它输出一个 ChatMessage
chain.output_schema.schema()
```

## 3、Stream

```python
for s in chain.stream({"topic": "bears"}):
    print(s.content, end="", flush=True)
```

## 4、Invoke

```python
chain.invoke({"topic": "bears"})
```

## 5、Batch

```python
chain.batch([{"topic": "bears"}, {"topic": "cats"}])
```

可以通过 `max_concurrency` 参数设置并发请求数

```python
chain.batch([{"topic": "bears"}, {"topic": "cats"}], config={"max_concurrency": 5})
```

## 6、Async Stream

```python
async for s in chain.astream({"topic": "bears"}):
    print(s.content, end="", flush=True)
```

## 7、Async Invoke

```python
await chain.ainvoke({"topic": "bears"})
```

## 8、Async Batch

```python
await chain.abatch([{"topic": "bears"}])
```